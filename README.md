
# Learned Pseudo Random Number Generator: LPRNG

"LPRNG" stands for Learned Pseudo Random Number Generator. It is a data-driven random number generator that leverages the power of Generative Adversarial Networks (GANs). The key objective of LPRNG is to train a model that can emulate the functionality of the Mersenne Twister, a widely used pseudo-random number generator. By doing so, LPRNG aims to create its own random number generators capable of passing the NIST (National Institute of Standards and Technology) random number tests, thereby ensuring the statistical robustness of the numbers it generates.

## Directory Structure

The project is organized as follows:

- `code/Learned_PRNG.ipynb`: This Jupyter Notebook contains the implementation code for the LPRNG. It is where you can find the detailed programming logic and methodology employed to create the pseudo-random number generator.

- `model/WGAN_based_model.pth`: This file is the pre-trained LPRNG model based on the WGAN (Wasserstein Generative Adversarial Network) architecture. The model stored in this file has been trained and is ready to be used for generating random numbers.

By exploring these directories, users can understand the implementation details and utilize the pre-trained model for generating pseudo-random numbers.



## Overview of the Study

![Schematic Overview of the Study](https://github.com/user-attachments/assets/d72ac9ba-291c-4ed1-9deb-44eb3a530ac2)

This figure provides a schematic overview of our study. We have developed a novel end-to-end model for random number generation, which we refer to as the Learned PRNG (LPRNG). Unlike simply enhancing existing methods, LPRNG introduces a fresh approach by incorporating a Generative Adversarial Network (GAN) that utilizes the Wasserstein distance. This is known as WGAN, a sophisticated version of the standard GAN.

### Key Features of LPRNG:

- **Learning from Mersenne Twister (MT):** LPRNG is trained on an inexhaustible supply of random numbers generated by the well-known MT PRNG algorithm.
  
- **NIST Test Suite Compliance:** The random numbers produced by LPRNG pass all the tests in the NIST test suite, adhering to the NIST recommended settings. This includes over 1,000 test runs, ensuring practical and reliable randomness.

- **Evaluating Robustness:** To test the robustness of LPRNG, we used seed values based on the cosine function, known for their poor randomness properties. This choice helps prevent information leakage during the learning phase.

- **Verification with NIST Suite:** The poor randomness of these seed values was confirmed using the NIST test suite.

- **Observing Learning Processes:** We carefully monitored the learning process of LPRNG, witnessing how it progressively enhances its capability to transform input seeds into numbers with improved randomness.

Our study presents a significant leap in random number generation, showcasing the potential of advanced machine learning techniques in this domain.


# Features

## Creating an Input Seed

The primary goal of this study is to generate random numbers using machine learning techniques, particularly Generative Adversarial Networks (GANs). A critical aspect of this process is the selection of the input seed. Contrary to typical approaches, in this study, the input seed should **not** inherently possess random properties. 

### Why Avoid Random Seeds?
Using random numbers as input seeds could lead to ambiguity in the results. Specifically, it would be unclear whether the GAN is actually learning to generate randomness or simply echoing the already random properties of the input seed. To ascertain that the GAN is genuinely learning to produce random numbers, we need to start with a seed that lacks randomness.

### Our Approach:
We chose input seeds with poor random properties, ideal for GAN learning. These seeds include sequences with regularity or periodicity, such as those generated by trigonometric functions.
```math
    \mathrm{input}_i = \cos(2R_1 \pi i + 2R_2 \pi) \times \frac{1}{2} + \frac{1}{2}, \quad i = 0, 1, \ldots, 35
```


### Addressing Batch-to-Batch Relationships:
GANs can't control the relationship between different batches of numbers. To mitigate this, we introduced variation by using different initial phases ($R_2$) and periods ($R_1$) of trigonometric functions for each batch. This approach helps in reducing similarities between sequences and provides a diverse range of inputs for GAN training.

The final selection of the input seed is represented in the following figure:
![Input Seed Representation](https://github.com/user-attachments/assets/0dd3cd54-47d4-42cb-bd68-4dafe76ccd37)

## Transition of Learning

![NIST Test Results During Learning](https://github.com/user-attachments/assets/b93f66c7-e5ab-434e-bd41-db315f443a4e)

This figure displays the NIST test results of random numbers generated at each step of the learning process. We conducted the NIST test after every 3,000 iterations, combining multiple batches of numbers generated at each step.

### Learning Phases:
We categorized the learning process into four distinct regions: W, X, Y, and Z.

- **Region W:** Improvement in NIST scores as learning progressed.
- **Region X:** Fluctuating NIST scores, with failures in several tests such as Frequency, CumulativeSums, and Runs.
- **Region Y:** Complete passing of the NIST test, yet occasional failures in certain tests like Frequency and CumulativeSums.
- **Region Z:** Not explicitly described but presumably represents the final learning stage.

### Observations:
- The model initially struggled to pass Frequency and CumulativeSums tests.
- After learning from MT random numbers, the model began passing the comprehensive 15-item NIST statistical test.
- Tables 2 and 3 (referenced but not shown here) detail the NIST test scores before and after learning.
- A, B, and C in Figureã€€(b) illustrate the growth in randomness.

### Key Insight:
Continuous learning posed challenges in consistently passing all NIST tests. However, we discovered that generating high-quality random numbers is feasible even with limited learning, as long as the output consists of multiple batches. The random numbers formed by connecting these batches successfully pass the NIST test.


# Requirement (Environment Used for This Calculation)

To replicate the environment used for our calculations, please ensure that your setup includes the following software and libraries with their specified versions:

- **Python**: Version 3.7.7
- **PyTorch**: Version 1.6.0
- **NumPy**: Version 1.19.1
- **CUDA** (specifically for PyTorch): Version 10.2
- **torchvision**: Version 0.7.0
- **matplotlib (pyplot)**: Version 3.3.1
- **imageio**: Version 2.4.1

It's important to match these versions to ensure compatibility and reproduce the results accurately.





# Usage

To use the `Learned_PRNG` Jupyter Notebook and conduct the NIST randomness test on the generated random numbers, follow these steps:

1. **Open the Notebook**:
   - Navigate to the `learned-prng/code/` directory.
   - Open the `Learned_PRNG.ipynb` Jupyter Notebook.

2. **Execute the Notebook**:
   - Run each cell in the notebook in sequence, from the top to the bottom.
   - The cells contain the code needed to generate random numbers using the learned pseudo-random number generator (PRNG).

3. **Analyze the Output**:
   - The notebook will output a sequence of random numbers upon execution.
   - Save this sequence for the next step.

4. **Perform the NIST Randomness Test**:
   - Access the NIST randomness test tutorial at the following URL: [https://csrc.nist.gov/projects/random-bit-generation/documentation-and-software].
   - Follow the tutorial's guidelines to apply the NIST randomness test to your sequence of random numbers.

By following these instructions, you can generate a sequence of random numbers using the `Learned_PRNG` notebook and assess their randomness using the NIST standards.




# Paper

## Learned Pseudo-Random Number Generator: WGAN-GP for Generating Statistically Robust Random Numbers

The paper detailing the research and findings can be accessed at the following DOI link:
[Learned Pseudo-Random Number Generator: WGAN-GP for Generating Statistically Robust Random Numbers](https://doi.org/10.1371/journal.pone.0287025)

### Authors
- Kiyoshiro Okada
- Katsuhiro Endo
- Kenji Yasuoka
- Shuichi Kurabayashi

### Author of This README
- **Kiyoshiro Okada**

### Affiliation
- **Cygames**

### Contact
- **E-mail**: [okada_kiyoshiro@cygames.co.jp](mailto:okada_kiyoshiro@cygames.co.jp)

# License

The "LPRNG" project is licensed under the [MIT License](https://en.wikipedia.org/wiki/MIT_License). This license permits free use, modification, distribution, and private use of the software, subject to the conditions outlined in the license.


